---
geometry: margin=2.5cm
header-includes: \usepackage[fontsize=15pt]{scrextend}
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

Extract from:  
Bradley Efron and Trevor Hastie  
*Computer Age Statistical Inference: Algorithms, Evidence, and Data Science  
Cambridge University Press, 2016  
https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf*</div>

&nbsp;


$\quad$Modern Bayesian practice uses various strategies to construct an appropriate “prior” $g(\mu)$ in the absence of prior experience, leaving many statisticians unconvinced by the resulting Bayesian inferences. Our second example illustrates the difficulty.


```{r, echo=FALSE}
library(kableExtra)
options(knitr.table.format = "latex")
```


**`r text_spec("Table 3.1", color = "teal")`** *Scores from two tests taken by 22 students, `r text_spec("mechanics", color = "green")` and `r text_spec("vectors", color = "green")`.*

```{r, echo=FALSE}
mechanics <- c(7,44,49,59,34,46,0,32,49,52,44,36,42,5,22,18,41,48,31,42,46,63)
vectors <- c(51,69,41,71,42,40,40,45,57,64,61,59,60,30,58,51,63,38,42,69,49,63)
n <- length(mechanics)
tbl <- rbind(t(mechanics),t(vectors))
rownames(tbl) <- c('mechanics', 'vectors')
colnames(tbl) <- as.factor(1:n)
kable(tbl[,1:(n/2)], format = "markdown", row.names = TRUE, align='c')
kable(tbl[,(n/2):n], format = "markdown", row.names = TRUE, align='c')
```

<br>
$\quad$Table 3.1 shows the scores on two tests, `r text_spec("mechanics", color = "green")` and `r text_spec("vectors", color = "green")`, achieved by $n=22$ students. The sample correlation coefficient between the two scores is $\widehat{\theta}=0.498$,

$$\widehat{\theta }=\sum_{i=1}^{22}(m_{i}-\overline{m})(v_{i}-\overline{v})/[\sum_{i=1}^{22}(m_{i}-\overline{m})^{2}\sum_{i=1}^{22}(v_{i}-\overline{v})^{2}]^{1/2}$$
with $m$ and $v$ short for `r text_spec("mechanics", color = "green")` and `r text_spec("vectors", color = "green")`, $\overline{m}$ and $\overline{v}$ their averages.

